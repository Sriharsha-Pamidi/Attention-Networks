{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "246b840e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "861cadc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(layers.Layer):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
    "        super(TransformerBlock, self).__init__()\n",
    "        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim) \n",
    "        self.ffn = keras.Sequential([layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),])\n",
    "        \n",
    "        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6) # Normalize the activations of the previous layer\n",
    "        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6) # Normalize the activations of the previous layer\n",
    "        self.dropout1 = layers.Dropout(rate)\n",
    "        self.dropout2 = layers.Dropout(rate)\n",
    "\n",
    "    def call(self, inputs, training):\n",
    "        attn_output = self.att(inputs, inputs)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(inputs + attn_output)\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        return self.layernorm2(out1 + ffn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fdef159",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputAndPositionEmbedding(layers.Layer):\n",
    "    def __init__(self, maxlen, vocab_size, embed_dim):\n",
    "        super(InputAndPositionEmbedding, self).__init__()\n",
    "        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n",
    "        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n",
    "\n",
    "    def call(self, x):\n",
    "        maxlen = tf.shape(x)[-1]\n",
    "        positions = tf.range(start=0, limit=maxlen, delta=1)\n",
    "        positions = self.pos_emb(positions)\n",
    "        x = self.token_emb(x)\n",
    "        return x + positions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def56cfa",
   "metadata": {},
   "source": [
    "IMDB DATASET: https://www.tensorflow.org/api_docs/python/tf/keras/datasets/imdb/load_data\n",
    "\n",
    "This is a dataset of 25,000 movies reviews from IMDB, labeled by sentiment (positive/negative). Reviews have been preprocessed, and each review is encoded as a list of word indexes (integers). For convenience, words are indexed by overall frequency in the dataset, so that for instance the integer \"3\" encodes the 3rd most frequent word in the data. This allows for quick filtering operations such as: \"only consider the top 10,000 most common words, but eliminate the top 20 most common words\".\n",
    "\n",
    "As a convention, \"0\" does not stand for a specific word, but instead is used to encode any unknown word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79195d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000  # top 20k words\n",
    "maxlen = 200  # first 200 words of movie review\n",
    "\n",
    "(x_train, y_train), (x_val, y_val) = keras.datasets.imdb.load_data(num_words=vocab_size)\n",
    "\n",
    "x_train = keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_val = keras.preprocessing.sequence.pad_sequences(x_val, maxlen=maxlen)\n",
    "\n",
    "print(len(x_train))\n",
    "print(len(x_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bd1300bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    1   14   47    8   30   31    7    4  249  108    7\n",
      "    4 5974   54   61  369   13   71  149   14   22  112    4 2401  311\n",
      "   12   16 3711   33   75   43 1829  296    4   86  320   35  534   19\n",
      "  263 4821 1301    4 1873   33   89   78   12   66   16    4  360    7\n",
      "    4   58  316  334   11    4 1716   43  645  662    8  257   85 1200\n",
      "   42 1228 2578   83   68 3912   15   36  165 1539  278   36   69    2\n",
      "  780    8  106   14 6905 1338   18    6   22   12  215   28  610   40\n",
      "    6   87  326   23 2300   21   23   22   12  272   40   57   31   11\n",
      "    4   22   47    6 2307   51    9  170   23  595  116  595 1352   13\n",
      "  191   79  638   89    2   14    9    8  106  607  624   35  534    6\n",
      "  227    7  129  113] 200\n"
     ]
    }
   ],
   "source": [
    "print(x_train[2],len(x_train[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9040ec8a",
   "metadata": {},
   "source": [
    "why attention ?\n",
    "\n",
    "The need for attention can be understood with a simple example. Let’s say “I went to movie this summer and i understood the emotions better considering the last time I saw this”. \n",
    "\n",
    "The last word “this” refers to the movie. But to understand this, remembering the first few parts is essential. \n",
    "To achieve this, the attention mechanism decides at each step of an input sequence which other parts of the sequence are important. \n",
    "\n",
    "In simple words, “You need context!”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "079aabec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 08:24:59.068736: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "embed_dim = 32  # Embedding size for each token\n",
    "num_heads = 2  # Number of attention heads\n",
    "ff_dim = 32  # Hidden layer size in feed forward network inside transformer\n",
    "\n",
    "inputs = layers.Input(shape=(maxlen,))\n",
    "\n",
    "embedding_layer = InputAndPositionEmbedding(maxlen, vocab_size, embed_dim)\n",
    "x = embedding_layer(inputs)\n",
    "\n",
    "transformer_block = TransformerBlock(embed_dim, num_heads, ff_dim)\n",
    "x = transformer_block(x)\n",
    "x = layers.GlobalAveragePooling1D()(x) # Global average pooling operation for temporal data.\n",
    "\n",
    "x = layers.Dropout(0.1)(x) # Applies Dropout to the input.\n",
    "\n",
    "x = layers.Dense(20, activation=\"relu\")(x)\n",
    "x = layers.Dropout(0.1)(x)\n",
    "\n",
    "outputs = layers.Dense(2, activation=\"softmax\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9eb2dac8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-15 08:24:59.771358: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "782/782 [==============================] - 68s 85ms/step - loss: 0.3875 - accuracy: 0.8170 - val_loss: 0.2958 - val_accuracy: 0.8736\n",
      "Epoch 2/2\n",
      "782/782 [==============================] - 63s 80ms/step - loss: 0.1970 - accuracy: 0.9249 - val_loss: 0.3168 - val_accuracy: 0.8651\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x12fdac130>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=2, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a21ff329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 200)]             0         \n",
      "_________________________________________________________________\n",
      "input_and_position_embedding (None, 200, 32)           646400    \n",
      "_________________________________________________________________\n",
      "transformer_block (Transform (None, 200, 32)           10656     \n",
      "_________________________________________________________________\n",
      "global_average_pooling1d (Gl (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 20)                660       \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2)                 42        \n",
      "=================================================================\n",
      "Total params: 657,758\n",
      "Trainable params: 657,758\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bf9982c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.92139137, 0.07860863],\n",
       "       [0.00417697, 0.995823  ],\n",
       "       [0.2971647 , 0.7028353 ],\n",
       "       ...,\n",
       "       [0.88604164, 0.11395831],\n",
       "       [0.9538809 , 0.04611911],\n",
       "       [0.2648748 , 0.73512524]], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(x_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bfdbdc9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     1,   591,   202,\n",
       "          14,    31,     6,   717,    10,    10, 18142, 10698,     5,\n",
       "           4,   360,     7,     4,   177,  5760,   394,   354,     4,\n",
       "         123,     9,  1035,  1035,  1035,    10,    10,    13,    92,\n",
       "         124,    89,   488,  7944,   100,    28,  1668,    14,    31,\n",
       "          23,    27,  7479,    29,   220,   468,     8,   124,    14,\n",
       "         286,   170,     8,   157,    46,     5,    27,   239,    16,\n",
       "         179, 15387,    38,    32,    25,  7944,   451,   202,    14,\n",
       "           6,   717], dtype=int32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c63373",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1397fd2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b07f7c30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "595f4109",
   "metadata": {},
   "source": [
    "Reference: https://aclanthology.org/2020.acl-main.312.pdf"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
